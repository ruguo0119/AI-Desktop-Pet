import json
import time
import os
import random
import uuid
import datetime
import chromadb
from chromadb.config import Settings
from services import AIService

class MemorySystem:
    def __init__(self):
        # 1. å‘é‡åº“ (å­˜ç»å†/å¯¹è¯ç‰‡æ®µ)
        self.chroma = chromadb.PersistentClient(path="./memory_db")
        self.episodic_col = self.chroma.get_or_create_collection("episodes")
        
        # 2. äº‹å®åº“ (å­˜å±æ€§ JSON)
        self.facts_path = "user_facts.json"
        self.facts = self._load_facts()

    def _load_facts(self):
        if os.path.exists(self.facts_path):
            with open(self.facts_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {}

    def _save_facts(self):
        with open(self.facts_path, 'w', encoding='utf-8') as f:
            json.dump(self.facts, f, ensure_ascii=False, indent=2)

    def get_fact_context(self):
        """æä¾›ç»™ä¸»è„‘çš„æ‰€æœ‰å·²çŸ¥ä¿¡æ¯"""
        facts_str = json.dumps(self.facts, ensure_ascii=False)
        return facts_str
    async def get_longmemory_context(self, user_text: str = None):
        memory_str = "" 
        if user_text:
            query_vec = await AIService.get_embedding(user_text)
            if query_vec:
                results = self.episodic_col.query(
                    query_embeddings=[query_vec],
                    n_results=3
                )
                if results['documents'] and results['documents'][0]:
                    docs = results['documents'][0]
                    metas = results['metadatas'][0]
                    distances = results['distances'][0] 
                    
                    # æ„å»ºä¸€ä¸ªåˆ—è¡¨æ¥å¤„ç†æ’åº
                    ranked_memories = []
                    now = datetime.datetime.now()
                    
                    for i, (doc, meta, dist) in enumerate(zip(docs, metas, distances)):
                        # åŸºç¡€åˆ†ï¼šå°†è·ç¦»è½¬åŒ–ä¸ºç›¸ä¼¼åº¦åˆ†æ•° (0~1ä¹‹é—´)
                        # è·ç¦»è¶Šå°åˆ†è¶Šé«˜ï¼ŒåŠ 1é˜²æ­¢é™¤é›¶
                        base_score = 1 / (1 + dist)      
                        # è·å–æ—¶é—´å¹¶è®¡ç®—å¤©æ•°å·®
                        date_str = meta.get('date', '1970-01-01')
                        try:
                            mem_date = datetime.datetime.strptime(date_str, "%Y-%m-%d")
                            days_diff = (now - mem_date).days
                        except:
                            days_diff = 9999   
                        time_boost = 1.0
                        if days_diff <= 3:
                            time_boost = 1.2
                        elif days_diff <= 30:
                            time_boost = 1.1
                            
                        final_score = base_score * time_boost
                        
                        ranked_memories.append({
                            "content": doc,
                            "date": date_str,
                            "score": final_score
                        })
                    
                    # æŒ‰æœ€ç»ˆåˆ†æ•°é‡æ–°æ’åºï¼Œå–å‰3
                    ranked_memories.sort(key=lambda x: x['score'], reverse=True)
                    top_k = ranked_memories[:3]

                    memory_str += "\nã€å…³è”å¾€äº‹ã€‘\n"
                    for mem in top_k:
                        print(f"è®°å¿†: {mem['content'][:10]}... | åŸå§‹åˆ†: {base_score:.3f} | æœ€ç»ˆåˆ†: {mem['score']:.3f}")
                        memory_str += f"- ({mem['date']}) {mem['content']}\n"
  
        else:
       
            total_count = self.episodic_col.count()
            if total_count > 0:
                random_offset = random.randint(0, total_count - 1)
           
                random_result = self.episodic_col.get(
                    limit=1,
                    offset=random_offset
                )
                
                if random_result['documents']:
                    doc = random_result['documents'][0]
              
                    meta = random_result['metadatas'][0] if random_result['metadatas'] else {}
                    date_str = meta.get('date', 'ä¹…è¿œçš„å›å¿†')
                    
                    memory_str += f"\nã€çªç„¶æƒ³èµ·ã€‘\n- ({date_str}) {doc}\n"
                
        return memory_str



    async def execute_updates(self, update_instruction: dict):
        """
        æ‰§è¡Œä¸»è„‘ä¸‹è¾¾çš„è®°å¿†æŒ‡ä»¤
        update_instruction ç»“æ„:
        {
            "new_facts": {"key": "value"},  // æ›´æ–°å±æ€§
            "new_episode": "ä»Šå¤©å‘ç”Ÿäº†..."   // å­˜å…¥ç»å†
        }
        """
     
        new_facts = update_instruction.get("new_facts")
        if new_facts:
            print(f"ğŸ§  ä¸»è„‘å†³å®šæ›´æ–°äº‹å®: {new_facts}")
            self.facts.update(new_facts)
            self._save_facts()

  
        new_episode = update_instruction.get("new_episode")
        if new_episode:
            print(f"ğŸ“… ä¸»è„‘å†³å®šè®°å½•ç»å†: {new_episode}")
            vector = await AIService.get_embedding(new_episode)
            if vector:
                self.episodic_col.add(
                    documents=[new_episode],
                    embeddings=[vector],
                    metadatas=[{
                        "timestamp": time.time(),
                        "date": datetime.datetime.now().strftime("%Y-%m-%d")
                    }],
                    ids=[str(uuid.uuid4())]
                )
                print(f"âœ… å­˜å…¥æˆåŠŸ! å½“å‰æ€»è®°å¿†æ•°: {self.episodic_col.count()}") 
            