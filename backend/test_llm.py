import asyncio
import os
from openai import AsyncOpenAI
from dotenv import load_dotenv

# 1. å¼ºåˆ¶é‡è½½ç¯å¢ƒå˜é‡
load_dotenv(override=True)

async def debug_gemini():
    api_key = os.getenv("LLM_API_KEY")
    base_url = os.getenv("LLM_BASE_URL")
    model = os.getenv("LLM_MODEL")

    print(f"ğŸ” æ£€æŸ¥é…ç½®:")
    print(f"   URL:   {base_url}")
    print(f"   Key:   {api_key[:8]}******") # æ£€æŸ¥ Key æ˜¯å¦è¯»å–æ­£ç¡®
    print(f"   Model: {model}")

    if not api_key or not base_url:
        print("âŒ é”™è¯¯: ç¯å¢ƒå˜é‡æœªè¯»å–åˆ°ï¼è¯·æ£€æŸ¥ .env æ–‡ä»¶è·¯å¾„")
        return

    client = AsyncOpenAI(api_key=api_key, base_url=base_url)

    print("\nğŸ“¡ æ­£åœ¨å‘é€è¯·æ±‚ (è¯·ç­‰å¾…)...")
    
    try:
        # ä½¿ç”¨æœ€åŸºç¡€çš„è°ƒç”¨ï¼Œå¼€å¯ full_response è°ƒè¯•
        response = await client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": "Say hello in English."}],
            max_tokens=50
        )
        
        print("\nâœ… è¯·æ±‚æˆåŠŸè¿”å›ï¼")
        print("------------------------------------------------")
        print(f"å®Œæ•´å“åº”å¯¹è±¡: {response}") # æ‰“å°æ•´ä¸ªå¯¹è±¡ï¼Œçœ‹é‡Œé¢åˆ°åº•æœ‰äº›å•¥
        print("------------------------------------------------")
        
        content = response.choices[0].message.content
        if not content:
            print("âš ï¸ è­¦å‘Š: Content å­—æ®µæ˜¯ç©ºçš„/Noneï¼")
            print("å¯èƒ½åŸå› : æ¨¡å‹åç§°ä¸æ”¯æŒï¼Œæˆ–è€…è§¦å‘äº†å†…å®¹è¿‡æ»¤")
        else:
            print(f"ğŸ’¬ å†…å®¹: {content}")

    except Exception as e:
        print(f"\nâŒ è¯·æ±‚ç‚¸äº†: {e}")

if __name__ == "__main__":
    asyncio.run(debug_gemini())